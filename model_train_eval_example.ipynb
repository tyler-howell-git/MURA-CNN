{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dec3607-4581-494a-83b8-65d89c3e00da",
   "metadata": {},
   "source": [
    "## Example Baseline CNN for Binary Classification on MURA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215658f0-24e4-4f69-8663-157be225c34e",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b077d64-258a-4b47-bea8-d93ed31311eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694120f8-73a8-4766-bad5-75b6947efe91",
   "metadata": {},
   "source": [
    "### Project Utility Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e79b5c9-b1ce-4150-96f9-10cf3b39300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mura_dataset import MURADataset\n",
    "from utils.transforms import get_train_transforms, get_val_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34793f9-7c55-4607-bad0-7942661970bc",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4dd81f-c34b-4e3a-a098-8a955e4830cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1) #first conv layer (input = 1 channel, output = 16 feature maps)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 128) #fully connected input dims (32 channels * 56 * 56 (after 2x pooling from 224))\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # [B, 16, 112, 112]\n",
    "        x = self.pool(F.relu(self.conv2(x))) # [B, 32, 56, 56]\n",
    "        x = x.view(x.size(0), -1) #flatten for fc layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.sigmoid(self.fc2(x)) #ouputs prob between 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febda6ed-7f85-4e71-bb69-1e75293ac42d",
   "metadata": {},
   "source": [
    "### Function to Load Train, Test, and Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89499293-ece9-465c-8d49-563c1cfce0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size=32):\n",
    "    train_dataset = MURADataset(\n",
    "        csv_file=\"data/splits/train_labeled_studies.csv\",\n",
    "        transform=get_train_transforms(),\n",
    "        root_dir=\"data/raw\"\n",
    "    )\n",
    "    val_dataset = MURADataset(\n",
    "        csv_file=\"data/splits/valid_labeled_studies.csv\",\n",
    "        transform=get_val_transforms(),\n",
    "        root_dir=\"data/raw\"\n",
    "    )\n",
    "    test_dataset = MURADataset(\n",
    "        csv_file=\"data/splits/valid_labeled_studies.csv\",\n",
    "        transform=get_val_transforms(),\n",
    "        root_dir=\"data/raw\"\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09268c-91af-4f14-afe2-953ac704cb0d",
   "metadata": {},
   "source": [
    "### Training Func for One Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a135de41-b895-4704-99ea-da2d32ad5392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for images, labels in tqdm(loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device) #[B] -> [B,1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images) #fwd pass\n",
    "        loss = criterion(outputs, labels) #binary CE loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    return running_loss / len(loader.dataset) #avg loss for epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730930a-8414-49ca-86c1-d9bf10abdd65",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1acd21f0-4646-4888-9e68-83f5f626e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_labels, all_probs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).cpu().numpy().flatten()\n",
    "            all_probs.extend(outputs)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    preds = [1 if p >= 0.5 else 0 for p in all_probs] #binary preds w/ 0.5 threshold between classes\n",
    "    acc = accuracy_score(all_labels, preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    return acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725745a-57f9-4358-bf59-502f81c3aa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [10:26<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 0.6608\n",
      "Val Acc:    0.5540, AUC: 0.6126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [08:50<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Train Loss: 0.6481\n",
      "Val Acc:    0.5824, AUC: 0.6416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [10:22<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Train Loss: 0.6397\n",
      "Val Acc:    0.5984, AUC: 0.6569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [09:37<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Train Loss: 0.6331\n",
      "Val Acc:    0.6218, AUC: 0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [10:23<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Train Loss: 0.6254\n",
      "Val Acc:    0.5962, AUC: 0.6679\n",
      "\n",
      "Test Accuracy: 0.5962, AUC: 0.6679\n",
      "Saved model weights -> models/baseline_cnn.pt\n",
      "Saved training curves -> evaluation/baseline_history.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os, numpy as np, torch\n",
    "\n",
    "# os.chdir(r\"c:\\Users\\ENOPARA\\code\\MachineLearning2025Project\\cnnradiographproject\")\n",
    "# print(\"WORKING DIR:\", os.getcwd())\n",
    "# print(\"Exists data/splits?:\", os.path.exists(\"data/splits\"))\n",
    "# print(\"Train CSV exists?:\", os.path.exists(\"data/splits/train_labeled_studies.csv\"))\n",
    "\n",
    "\"\"\"Tweaked to save evaluation results\"\"\"\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # if gpu avail\n",
    "    train_loader, val_loader, test_loader = get_loaders()  # load data\n",
    "\n",
    "    model = BaselineCNN().to(device)  # model init\n",
    "    criterion = nn.BCELoss()          # loss init\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)  # optim init\n",
    "\n",
    "    # lists to store history for saving later\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    val_aucs = []\n",
    "\n",
    "    # train for 5 epochs\n",
    "    for epoch in range(5):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_acc, val_auc = evaluate(model, val_loader, device)\n",
    "\n",
    "        # store history\n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_aucs.append(val_auc)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Acc:    {val_acc:.4f}, AUC: {val_auc:.4f}\")\n",
    "\n",
    "    # perform eval on the test set\n",
    "    test_acc, test_auc = evaluate(model, test_loader, device)\n",
    "    print(f\"\\nTest Accuracy: {test_acc:.4f}, AUC: {test_auc:.4f}\")\n",
    "\n",
    "    # === SAVE WEIGHTS + HISTORY ===\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"evaluation\", exist_ok=True)\n",
    "\n",
    "    torch.save(model.state_dict(), \"models/baseline_cnn.pt\")\n",
    "    print(\"Saved model weights -> models/baseline_cnn.pt\")\n",
    "\n",
    "    np.savez(\n",
    "        \"evaluation/baseline_history.npz\",\n",
    "        train_loss=np.array(train_losses),\n",
    "        val_acc=np.array(val_accuracies),\n",
    "        val_auc=np.array(val_aucs),\n",
    "        test_acc=test_acc,\n",
    "        test_auc=test_auc,\n",
    "    )\n",
    "    print(\"Saved training curves -> evaluation/baseline_history.npz\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7dd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"storing numbers from running su's code for later use\"\"\"\n",
    "# Manually copy from the console output above:\n",
    "train_losses = [\n",
    "    0.6610,\n",
    "    0.6458,\n",
    "    0.6367,\n",
    "    0.6276,\n",
    "    0.6198,\n",
    "]\n",
    "\n",
    "val_accuracies = [\n",
    "    0.5543,\n",
    "    0.5630,\n",
    "    0.5834,\n",
    "    0.5940,\n",
    "    0.6215,\n",
    "]\n",
    "\n",
    "val_aucs = [\n",
    "    0.6247,\n",
    "    0.6500,\n",
    "    0.6629,\n",
    "    0.6657,\n",
    "    0.6760,\n",
    "]\n",
    "\n",
    "# final test metrics from the last line printed\n",
    "test_acc = 0.6215\n",
    "test_auc = 0.6763\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce39a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mura-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
