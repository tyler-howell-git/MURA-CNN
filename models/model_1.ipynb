{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d15052-0e7f-4f3c-bdbd-1e0fe9008509",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Training Model 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c635cc8",
   "metadata": {},
   "source": [
    "Imports Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c657e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7003eabd-792c-4508-8a3b-9aa2a84c4dd5",
   "metadata": {},
   "source": [
    "Project Utility Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa1c5a3-567b-4692-8b59-59e6209daf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds root directory to sys.path\n",
    "import sys, os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from utils.mura_dataset import MURADataset\n",
    "from utils.transforms import get_train_transforms, get_val_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d9dd3-855f-4f7e-bf46-143243ff8e3c",
   "metadata": {},
   "source": [
    "Model Definition - Deeper CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb59908-6c9a-4da7-86c2-502c3d9be552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defines convolutional layers in blocks \n",
    "\n",
    "        # --- Block 1 ---\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), #first conv layer (input = 1 channel, output = 32 feature maps)\n",
    "            nn.BatchNorm2d(32), # Batch normalization stabilizing and accelerates training.  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), \n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # -- Block 2 ---\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2), \n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        # --- Block 3 -- \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling \n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Final classifier (fully connected layers)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return torch.sigmoid(self.fc(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddac3ff",
   "metadata": {},
   "source": [
    "Data Loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26707f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size=32):\n",
    "    train_dataset = MURADataset(\n",
    "        csv_file=\"../data/splits/train_labeled_studies_split.csv\",\n",
    "        transform=get_train_transforms(),\n",
    "        root_dir=\"../data/raw\"\n",
    "    )\n",
    "    val_dataset = MURADataset(\n",
    "        csv_file=\"../data/splits/val_labeled_studies_split.csv\",\n",
    "        transform=get_val_transforms(),\n",
    "        root_dir=\"../data/raw\"\n",
    "    )\n",
    "    test_dataset = MURADataset(\n",
    "        csv_file=\"../data/splits/valid_labeled_studies.csv\",\n",
    "        transform=get_val_transforms(),\n",
    "        root_dir=\"../data/raw\"\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca294e",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b63fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device) #[B] -> [B,1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images) #fwd pass\n",
    "        loss = criterion(outputs, labels) #binary CE loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    return running_loss / len(loader.dataset) #avg loss for epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bbd0e1",
   "metadata": {},
   "source": [
    "Evaluation Function/Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2965defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_labels, all_probs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).cpu().numpy().flatten()\n",
    "            all_probs.extend(outputs)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    preds = [1 if p >= 0.5 else 0 for p in all_probs] #binary preds w/ 0.5 threshold between classes\n",
    "    \n",
    "    acc = accuracy_score(all_labels, preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    return acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab0e4fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 921/921 [29:17<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 0.6653\n",
      "Val Acc:    0.5430, AUC: 0.6048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 921/921 [27:09<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Train Loss: 0.6586\n",
      "Val Acc:    0.5607, AUC: 0.6219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 921/921 [27:06<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Train Loss: 0.6541\n",
      "Val Acc:    0.5861, AUC: 0.6264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 921/921 [27:21<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Train Loss: 0.6502\n",
      "Val Acc:    0.5945, AUC: 0.6342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 921/921 [27:25<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Train Loss: 0.6481\n",
      "Val Acc:    0.6094, AUC: 0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 921/921 [26:41<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "Train Loss: 0.6461\n",
      "Val Acc:    0.6146, AUC: 0.6454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 921/921 [27:27<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "Train Loss: 0.6441\n",
      "Val Acc:    0.6178, AUC: 0.6479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 921/921 [27:58<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "Train Loss: 0.6421\n",
      "Val Acc:    0.5843, AUC: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 921/921 [25:58<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "Train Loss: 0.6417\n",
      "Val Acc:    0.5977, AUC: 0.6453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 921/921 [27:53<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "Train Loss: 0.6404\n",
      "Val Acc:    0.6140, AUC: 0.6506\n",
      "\n",
      "Test Accuracy: 0.5952, AUC: 0.6437\n",
      "Saved Model 1 weights -> models/model1_10ep.pt\n",
      "Saved Model 1 training history -> evaluation/model1_10ep_history.npz\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, torch\n",
    "# history containers for Model 1 (10-epoch deeper CNN)\n",
    "train_losses_model1 = []\n",
    "val_accs_model1 = []\n",
    "val_aucs_model1 = []\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # if gpu avail\n",
    "    train_loader, val_loader, test_loader = get_loaders()  # load data\n",
    "\n",
    "    model = DeeperCNN().to(device)        # model init \n",
    "    criterion = nn.BCELoss()              # loss init\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)  # optim init\n",
    "\n",
    "    #train for 10 epochs \n",
    "    for epoch in range(10):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_acc, val_auc = evaluate(model, val_loader, device)\n",
    "\n",
    "        # store history for plotting later\n",
    "        train_losses_model1.append(train_loss)\n",
    "        val_accs_model1.append(val_acc)\n",
    "        val_aucs_model1.append(val_auc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Acc:    {val_acc:.4f}, AUC: {val_auc:.4f}\")\n",
    "\n",
    "    #final eval on the test set \n",
    "    test_acc, test_auc = evaluate(model, test_loader, device)\n",
    "    print(f\"\\nTest Accuracy: {test_acc:.4f}, AUC: {test_auc:.4f}\")\n",
    "\n",
    "    # SAVE WEIGHTS + HISTORY FOR MODEL 1\n",
    "   \n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"evaluation\", exist_ok=True)\n",
    "\n",
    "    # save model 1 weights\n",
    "    torch.save(model.state_dict(), \"models/model1_10ep.pt\")\n",
    "    print(\"Saved Model 1 weights -> models/model1_10ep.pt\")\n",
    "\n",
    "    # save training curves + test metrics (keys match baseline file)\n",
    "    np.savez(\n",
    "        \"evaluation/model1_10ep_history.npz\",\n",
    "        train_loss=np.array(train_losses_model1),\n",
    "        val_acc=np.array(val_accs_model1),\n",
    "        val_auc=np.array(val_aucs_model1),\n",
    "        test_acc=test_acc,\n",
    "        test_auc=test_auc,\n",
    "    )\n",
    "    print(\"Saved Model 1 training history -> evaluation/model1_10ep_history.npz\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bed87-ebbd-4708-8a9e-37f99e1e89b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mura-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
